{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP():\n",
    "    def __init__(self, listOfLayer, activationFunction = \"ReLU\"):\n",
    "        self.numLayers = len(listOfLayer)\n",
    "        self.layers = [{} for i in range(self.numLayers)]\n",
    "        self.activationFunction = activationFunction\n",
    "        \n",
    "        for i in range(1, self.numLayers):\n",
    "            self.layers[i][\"W\"] = np.random.randn(listOfLayer[i], listOfLayer[i-1])  # weight (random)\n",
    "            self.layers[i][\"b\"] = np.random.randn(listOfLayer[i], 1)  # bias (random)\n",
    "            \n",
    "        self.lossList = []  # loss value trace\n",
    "        return\n",
    "    \n",
    "    def activation(self, z, actType=\"sigmoid\"):\n",
    "        if actType == \"sigmoid\":\n",
    "            a = 1.0 / (1.0 + np.exp(-z))\n",
    "        elif actType == \"ReLU\":\n",
    "            a = np.where(z < 0.0, 0.0, z)  \n",
    "            \n",
    "        return a\n",
    "    \n",
    "    def inverseActivation(self, a, actType=\"sigmoid\"):\n",
    "        if actType == \"sigmoid\":\n",
    "            dzda = a * (1.0 - a)\n",
    "        elif actType == \"ReLU\":\n",
    "            dzda = np.where(a > 0, 1.0, a)\n",
    "            \n",
    "        return dzda\n",
    "    \n",
    "    def forward(self, x, bs):\n",
    "        self.layers[0][\"a\"] = np.copy(x)\n",
    "        \n",
    "        #Linear equation\n",
    "        for i in range(1, self.numLayers):\n",
    "            self.layers[i][\"z\"] = self.layers[i][\"W\"].dot(self.layers[i-1][\"a\"]) + self.layers[i][\"b\"]\n",
    "            actType = \"sigmoid\" if i == self.numLayers - 1 else self.activationFunction\n",
    "            self.layers[i][\"a\"] = self.activation(self.layers[i][\"z\"], actType)\n",
    "            \n",
    "        # last layer L does not use activation function    \n",
    "        self.p = self.softmax(self.layers[-1][\"a\"])\n",
    "        self.yHat = np.zeros(self.p.shape, dtype=int)  # predicted answers\n",
    "        for j, i in enumerate(self.p.argmax(axis=0)):\n",
    "            self.yHat[i, j] = 1\n",
    "        return\n",
    "    \n",
    "    def softmax(self, a):\n",
    "        expa = np.exp(a)\n",
    "        return (expa / np.sum(expa, axis=0))\n",
    "    \n",
    "    def loss(self, y):\n",
    "        return np.mean(- np.log(self.p) * y)\n",
    "    \n",
    "    def backprop(self, y, bs):\n",
    "        # Derivative of loss function\n",
    "        self.dJdp = - y / self.p\n",
    "        \n",
    "        # Derivative of softmax\n",
    "        dpda = np.zeros((self.p.shape[0], self.p.shape[0], bs)) # a and p are of same dimension\n",
    "        for b in range(bs):\n",
    "            for i in range(dpda.shape[0]):\n",
    "                for j in range(dpda.shape[1]):\n",
    "                    dpda[i, j, b] = (self.p[i, b] - self.p[i, b] ** 2) if i == j else - self.p[i, b] * self.p[j, b]\n",
    "        \n",
    "        self.layers[-1][\"dJda\"] = np.zeros(self.layers[-1][\"a\"].shape)\n",
    "        for b in range(bs):\n",
    "            self.layers[-1][\"dJda\"][:, b] = dpda[:, :, b].dot(self.dJdp[:, b])\n",
    "          \n",
    "        for i in range(self.numLayers - 1, 0, -1):\n",
    "            actType = \"sigmoid\" if i == self.numLayers - 1 else self.activationFunction\n",
    "            self.layers[i][\"dJdz\"] = self.inverseActivation(self.layers[i][\"a\"], actType) * self.layers[i][\"dJda\"]\n",
    "            self.layers[i][\"dJdb\"] = np.mean(self.layers[i][\"dJdz\"], axis = 1).reshape(self.layers[i][\"b\"].shape)\n",
    "            self.layers[i][\"dJdW\"] = self.layers[i][\"dJdz\"].dot(self.layers[i-1][\"a\"].T) / bs\n",
    "            self.layers[i-1][\"dJda\"] = self.layers[i][\"W\"].T.dot(self.layers[i][\"dJdz\"])\n",
    "        return\n",
    "    \n",
    "    def update(self, lr):\n",
    "        # lr: learning rate\n",
    "        for i in range(1, self.numLayers):\n",
    "            self.layers[i][\"W\"] -= lr * self.layers[i][\"dJdW\"]\n",
    "            self.layers[i][\"b\"] -= lr * self.layers[i][\"dJdb\"]\n",
    "        return\n",
    "    \n",
    "    def shuffle(self, a, b):\n",
    "        shuffled_a = np.copy(a)\n",
    "        shuffled_b = np.copy(b)\n",
    "        permutation = np.random.permutation(a.shape[1])\n",
    "        for oldindex, newindex in enumerate(permutation):\n",
    "            shuffled_a[:, oldindex] = a[:, newindex]\n",
    "            shuffled_b[:, oldindex] = b[:, newindex]\n",
    "        return shuffled_a, shuffled_b\n",
    "        \n",
    "    def train(self, trainX, trainY, numEpoch=1, lr=0.01, bs=2):  \n",
    "        for e in range(numEpoch):\n",
    "            shuffled_trainX, shuffled_trainY = self.shuffle(trainX, trainY)\n",
    "            for i in range(trainX.shape[1] // bs):\n",
    "                x = shuffled_trainX[:, i*bs : (i+1)*bs]\n",
    "                y = shuffled_trainY[:, i*bs : (i+1)*bs]\n",
    "                self.forward(x, bs)\n",
    "                self.lossList.append(self.loss(y))\n",
    "                self.backprop(y, bs)\n",
    "                self.update(lr)\n",
    "                \n",
    "        self.finalX = shuffled_trainX\n",
    "        self.finalY = shuffled_trainY\n",
    "        return      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward\n",
    "Linear equation: $$z^l = w^l a^{l-1} + b^l, \\forall l \\neq 0$$\n",
    "ReLU activation function: $$a^l_i = \\max(z^l_i, 0), \\forall i, 0<l<output\\_layer$$\n",
    "Softmax function: $$p_i = \\frac{e^{z^L_i}}{\\Sigma_j e^{z^L_j}}, \\forall i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward\n",
    "Corss entropy loss function: $$J = -\\sum_i (y_i \\ln p_i + (1-y_i) \\ln (1-p_i))$$\n",
    "Derivative of cross entropy loss function: $$\\frac{\\partial J}{\\partial p} = (\\frac{1}{1-y-p})^\\text{T} = dJdp$$\n",
    "Derivative of softmax: $$\\frac{\\partial p}{\\partial z^L} = \\text{diag}(p)-pp^\\text{T} = dpdz$$\n",
    "Derivative of ReLU activation function: $$\\text{diag}^{-1}(\\frac{\\partial a^l}{\\partial z^l}) = z^l > 0 ? 1:0 = diag\\_inv\\_dadz[l]$$\n",
    "Recursive formula by chain rule: \n",
    "$$\\frac{\\partial J}{\\partial z^L} = \\frac{\\partial J}{\\partial p} \\frac{\\partial p}{\\partial z^L} \\\\\n",
    "\\Rightarrow dJdz[last\\_layer] = dJdp \\times dpdz$$\n",
    "$$\\frac{\\partial J}{\\partial z^l} = (\\frac{\\partial J}{\\partial z^{l+1}} w^{l+1}) \\otimes {\\text{diag}^{-1}(\\frac{\\partial a^l}{\\partial z^l})}^\\text{T} \\\\\n",
    "\\Rightarrow dJdz[l] = (dJdz[l+1] \\times w[l+1]) \\otimes diag\\_inv\\_dadz[l]^\\text{T}$$\n",
    "Finally... \n",
    "$$\\frac{\\partial J}{\\partial w^l} = a^{l-1} \\frac{\\partial J}{\\partial z^l} = dJdw[l]$$\n",
    "$$\\frac{\\partial J}{\\partial b^l} = \\frac{\\partial J}{\\partial z^l} = dJdb[l]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training block\n",
    "### train 60k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"mnist_train_60000.csv\", \"r\")\n",
    "train_60k = f.readlines()\n",
    "f.close()\n",
    "\n",
    "x = [] \n",
    "y = []  # answers\n",
    "\n",
    "for line in train_60k:\n",
    "    linepixels = [int(pixel) for pixel in line.split(\",\")]\n",
    "    x.append(linepixels[1:])\n",
    "    y.append(linepixels[0])\n",
    "    \n",
    "train_x_60k = np.array(x).T / 256.0\n",
    "train_y_60k = np.zeros((10, len(y)), dtype=int)\n",
    "for i in range(len(y)):\n",
    "    train_y_60k[y[i], i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnMNIST60k = MLP([784, 80, 10], activationFunction = \"sigmoid\")\n",
    "nnMNIST60k.train(train_x_60k, train_y_60k, numEpoch=1000, lr=0.2, bs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnMNIST60k.forward(train_x_60k, 60000)\n",
    "predicted = nnMNIST60k.yHat.argmax(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy= 0.9419333333333333\n"
     ]
    }
   ],
   "source": [
    "y_ = np.array(y)\n",
    "print(\"Training Accuracy=\", np.sum(y_ == predicted) / y_.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing block\n",
    "### test 10k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2 = open(\"mnist_test_10000.csv\", \"r\")\n",
    "test_10k = f2.readlines()\n",
    "f2.close()\n",
    "\n",
    "x_test = [] \n",
    "y_test = []  # answers\n",
    "\n",
    "for line in test_10k:\n",
    "    linepixels = [int(pixel) for pixel in line.split(\",\")]\n",
    "    x_test.append(linepixels[1:])\n",
    "    y_test.append(linepixels[0])\n",
    "\n",
    "test_x_10k = np.array(x_test).T / 256.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy= 0.9375\n"
     ]
    }
   ],
   "source": [
    "nnMNIST60k.forward(test_x_10k, 10000)\n",
    "predicted_t = nnMNIST60k.yHat.argmax(axis=0)\n",
    "\n",
    "y_t = np.array(y_test)\n",
    "print(\"Testing Accuracy=\", np.sum(y_t == predicted_t) / y_t.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
